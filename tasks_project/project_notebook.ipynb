{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Functionality User Sample\n",
    "\n",
    "In order to help the research team find Gtmhub users to interview who use the task functionality, the data science and analytics team gathered, transformed, and extracted data on users that met their research criteria. More on the research plan [here](https://dovetailapp.com/projects/2IUhqbkGJ73oTG1mfsTIRq/readme).\n",
    "\n",
    "Steps:\n",
    "1. Get User and Account Data from Azure Data Lake.\n",
    "    - This data comes from the Gtmhub Raw set of data in Gtmhub. \n",
    "    - There are two sets of data, one from EU and one from US, so we combine them.\n",
    "2. Clean up and filter the user and account data.\n",
    "    - Remove unnecessary fields.\n",
    "    - Filter users for english speakers, active, and created greater than 6 months ago.\n",
    "    - Filter accounts for only active accounts.\n",
    "3. Get Backend Users from Redshift backend schema (in Azure Data Lake).\n",
    "    - These users contain additional information that the raw set of users do not contain (e.g., email, name, etc.).\n",
    "4. Clean up Backend Users and merge with Gtmhub Raw users.\n",
    "5. Join user and account data.\n",
    "6. Get task related data from Azure Data Lake (Redshift backend schema).\n",
    "    - Three different event tables: task_created, task_modified, task_deleted.\n",
    "7. Group and combine task related data by user.\n",
    "8. Join task data with user data.\n",
    "9. Get HubSpot contacts and companies.\n",
    "    - Contacts come from a separate script, `contacts.py` in the hubspot_tap repository.\n",
    "    - Companies come from a separate script, `companies.py` in the hubspot_tap repository.\n",
    "10. Clean up and combine the HubSpot contacts and companies.\n",
    "11. Get Chargebee subscriptions.\n",
    "    - This data comes from the chargebee_rest_subscriptions_all table from data sources in Gtmhub insights.\n",
    "    - SQL query is below.\n",
    "12. Join subscription data with user data.\n",
    "13. Join Hubspot data with user data.\n",
    "14. Output file to csv for delivery to research team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment vars\n",
    "load_dotenv()\n",
    "connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate blob service client\n",
    "try:\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "except Exception as e:\n",
    "    print(f'Unable to connect to BlobServiceClient: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_blob_as_df(blob_client, container, blob_path):\n",
    "    \"\"\"\n",
    "    Get a JSON blob from Azure and read it into a pandas dataframe.\n",
    "    Params:\n",
    "        :blob_client (BlobServiceClient object): Azure blob service client object\n",
    "        :container (str): Name of the Azure storage container\n",
    "        :blob_path (str): Name of the Azure blob\n",
    "    \"\"\"\n",
    "    blob_client_instance = blob_client.get_blob_client(container, blob_path)\n",
    "    streamdownloader = blob_client_instance.download_blob()\n",
    "    file_reader = json.loads(streamdownloader.readall())\n",
    "    df = pd.DataFrame(file_reader)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get users and accounts\n",
    "eu_users = get_json_blob_as_df(blob_service_client, \"researchanalyticsinsights\", \"Unprocessed/Gtmhub/2022/02/24/gtmhubrawuserseu.json\")\n",
    "us_users = get_json_blob_as_df(blob_service_client, \"researchanalyticsinsights\", \"Unprocessed/Gtmhub/2022/02/24/gtmhubrawusersus.json\")\n",
    "eu_accounts = get_json_blob_as_df(blob_service_client, \"researchanalyticsinsights\", \"Unprocessed/Gtmhub/2022/02/24/gtmhubrawaccountseu.json\")\n",
    "us_accounts = get_json_blob_as_df(blob_service_client, \"researchanalyticsinsights\", \"Unprocessed/Gtmhub/2022/02/24/gtmhubrawaccountsus.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine user & account dfs\n",
    "users_df = pd.concat([eu_users, us_users])\n",
    "accounts_df = pd.concat([eu_accounts, us_accounts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unneeded columns\n",
    "users_df = users_df.drop(['clientid', 'additionalinvitationsleft', 'data_source_id', 'sync_date'], axis=1)\n",
    "# Remove non-english and inactive users\n",
    "users_df = users_df[(users_df['language'] == 'english') & (users_df['isactive'] == True)]\n",
    "# Remove users if they were created less that 60 days ago\n",
    "users_df['datecreated'] = pd.to_datetime(users_df['datecreated'], format='%Y-%m-%dT%H:%M:%S', errors='coerce')\n",
    "six_months_ago = datetime.datetime.today() - datetime.timedelta(days=60)\n",
    "users_df = users_df[users_df['datecreated'] < six_months_ago]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "accounts_df = accounts_df.drop(['type', 'trialends', 'ownerid', 'edition', 'planid', 'hasslackintegration', 'settings', 'data_source_id', 'sync_date'], axis=1)\n",
    "# Keep active accounts\n",
    "accounts_df = accounts_df[accounts_df['isactive'] == True]\n",
    "# Fix datetime\n",
    "accounts_df['datecreated'] = pd.to_datetime(accounts_df['datecreated'], format='%Y-%m-%dT%H:%M:%S', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prefixes for table clarity\n",
    "users_df = users_df.add_prefix('user_')\n",
    "accounts_df = accounts_df.add_prefix('account_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of users per account\n",
    "account_sum = accounts_df.merge(users_df, how='inner', left_on='account_id', right_on='user_accountid')\n",
    "account_sum = account_sum.groupby('account_id')['user_id'].count().reset_index().rename(columns={'user_id': 'user_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get backend users\n",
    "backendusers_df = get_json_blob_as_df(blob_service_client, \"researchanalyticsinsights\", \"Unprocessed/Redshift/2022/02/24/backendusers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backendusers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backendusers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "backendusers_df = backendusers_df.drop(['received_at', 'uuid', 'editionname', 'account_id', 'account_name', 'accountstatus', 'avatar', 'context_library_name', 'company_account_status', 'uuid_ts', 'accountcreated', 'company_id', 'context_library_version', 'trialends', 'company_plan', 'created_at', 'editionplanid', 'context_group_id', 'deleted', 'company_status', 'status', 'company_edition', 'experiments', 'is_primary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prefix\n",
    "backendusers_df = backendusers_df.add_prefix('backenduser_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join backend with users\n",
    "user_df = users_df.merge(backendusers_df, how='inner', left_on='user_id', right_on='backenduser_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join sum with accounts\n",
    "account_df = accounts_df.merge(account_sum, how='inner', left_on='account_id', right_on='account_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop additional user columns\n",
    "user_df = user_df.drop(['user_isactive', 'backenduser_id', 'backenduser_company_created_at', 'backenduser_last_name', 'backenduser_first_name', 'backenduser_roles', 'user_language'], axis=1)\n",
    "# Drop additional account columns\n",
    "account_df = account_df.drop(['account_language', 'account_isactive'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join users and accounts\n",
    "df = user_df.merge(account_df, how='inner', left_on='user_accountid', right_on='account_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate column\n",
    "df = df.drop(['user_accountid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get task information\n",
    "task_created = get_json_blob_as_df(blob_service_client, \"researchanalyticsinsights\", \"Unprocessed/Redshift/2022/02/24/backendtask_created.json\")\n",
    "task_deleted = get_json_blob_as_df(blob_service_client, \"researchanalyticsinsights\", \"Unprocessed/Redshift/2022/02/24/backendtask_deleted.json\")\n",
    "task_modified = get_json_blob_as_df(blob_service_client, \"researchanalyticsinsights\", \"Unprocessed/Redshift/2022/02/24/backendtask_modified.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task_created by user df\n",
    "task_created = task_created.groupby('user_id')['id'].count().reset_index().rename(columns={'id': 'tasks_created'})\n",
    "task_created.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task_deleted by user df\n",
    "task_deleted = task_deleted.groupby('user_id')['id'].count().reset_index().rename(columns={'id': 'tasks_deleted'})\n",
    "task_deleted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task_modified by user df\n",
    "task_modified = task_modified.groupby('user_id')['id'].count().reset_index().rename(columns={'id': 'tasks_modified'})\n",
    "task_modified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge task_* dfs\n",
    "tasks_df = task_created.merge(task_deleted, how='outer', left_on='user_id', right_on='user_id')\n",
    "tasks_df = tasks_df.merge(task_modified, how='outer', left_on='user_id', right_on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN to 0\n",
    "tasks_df = tasks_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tasks with users\n",
    "df = df.merge(tasks_df, how='inner', left_on='user_id', right_on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df = df[~df.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HubSpot Contacts\n",
    "hs_contacts = pd.read_json('hubspot_contacts.json')\n",
    "# Explode properties\n",
    "hs_contacts = hs_contacts.join(hs_contacts.properties.apply(pd.Series))\n",
    "# Keep necessary columns\n",
    "hs_contacts = hs_contacts[['associatedcompanyid', 'email', 'hs_object_id', 'jobtitle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HubSpot Companies\n",
    "hs_companies = pd.read_json('hubspot_companies.json')\n",
    "# Explode properties\n",
    "hs_companies = hs_companies.join(hs_companies.properties.apply(pd.Series))\n",
    "# Keep necessary columns\n",
    "hs_companies = hs_companies[['hs_object_id', 'annualrevenue', 'industry', 'numberofemployees', 'website']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hubspot contacts with companies \n",
    "hubspot = hs_contacts.merge(hs_companies, how='left', left_on='associatedcompanyid', right_on='hs_object_id')\n",
    "hubspot = hubspot.drop(['associatedcompanyid', 'hs_object_id_x', 'hs_object_id_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubspot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsription SQL\n",
    "```\n",
    "SELECT\n",
    "    subscription_id,\n",
    "    subscription_mrr\n",
    "FROM chargebee_rest_subscriptions_all\n",
    "WHERE subscription_id IN (<list-of-subscription-ids-from-df>)\n",
    "ORDER BY subscription_id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subscriptions\n",
    "subscriptions = pd.read_csv('subscriptions.csv')\n",
    "# Remove unnecessary columns\n",
    "subscriptions = subscriptions.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge subscriptions with users\n",
    "df = df.merge(subscriptions, how='inner', left_on='account_subscriptionid', right_on='subscription_id')\n",
    "# Remove $0 subscriptions\n",
    "df = df[df.subscription_mrr > 0]\n",
    "# Remove gtmhub & primeholding users\n",
    "df = df[~df.backenduser_email.str.contains('primeholding')]\n",
    "df = df[~df.backenduser_email.str.contains('gtmhub')]\n",
    "# Remove unnecessary columns\n",
    "df = df.drop(['account_subscriptionid', 'subscription_id', 'subscription_mrr', 'user_id', 'account_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge users with HubSpot data\n",
    "df = df.merge(hubspot, how='left', left_on='backenduser_email', right_on='email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write user sample to CSV\n",
    "df.to_csv('task_user_sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ad9926f3f6540df89b17dec6d6ff8a3327446831cf9e1e2a75d60bbda191d0b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
